{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "\n",
    "# new version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "p=2 #nbr of classes\n",
    "k=256 #dimension of the dictionary\n",
    "n=100 # dimension of the signal\n",
    "\n",
    "#random variables\n",
    "alpha_0 = np.random.randn(k)\n",
    "w = np.random.randn(k, 1)\n",
    "b = np.random.randn(1)\n",
    "l = 1\n",
    "D = np.random.randn(n, k)\n",
    "signals = np.random.randn(10, n)\n",
    "lambda_0 = 1 #see later how to choose this\n",
    "\n",
    "mu =np.linspace(0, 1, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the case of binary classification\n",
    "\n",
    "def function_S(alpha, x, D, w, b, lambda_0, lambda_1, pos=True, method=\"linear\"):\n",
    "    if method == \"linear\":\n",
    "        if pos:\n",
    "            logistic_loss = np.log(1 + np.exp(-(w.T @ alpha + b)))\n",
    "        else:\n",
    "            logistic_loss = np.log(1 + np.exp(w.T @ alpha + b))\n",
    "    else:\n",
    "        print(\"Method not implemented\")\n",
    "        return np.nan\n",
    "\n",
    "    reg_1 = lambda_0 * np.linalg.norm(x - D @ alpha, ord=2)**2\n",
    "    reg_2 = lambda_1 * np.linalg.norm(alpha, ord=1)\n",
    "\n",
    "    return logistic_loss + reg_1 + reg_2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def supervised_sparse_coding(alpha, signals, D, w, b):\n",
    "    \"\"\"Supervised sparse coding step for 2 classes\"\"\"\n",
    "\n",
    "    n = signals.shape[0] #nbr of signals\n",
    "    size_dict = D.shape[1] #dictionary size\n",
    "    lambda_0 = 1\n",
    "    lambda_1 = 0.15\n",
    "\n",
    "    alpha_opt_pos = np.zeros((n, size_dict))\n",
    "    alpha_opt_neg = np.zeros((n, size_dict))\n",
    "\n",
    "    alpha_pos = alpha\n",
    "    alpha_neg = alpha\n",
    "\n",
    "    for i in range(n):\n",
    "        print(i)\n",
    "        x = signals[i, :]\n",
    "        # optimize on alpha S_neg(-1 f(alpha, x_i, D, theta)\n",
    "        alpha_temp_neg = scipy.optimize.minimize(function_S, alpha_neg, args= (x, D, w, b, lambda_0, lambda_1, False), tol=1e-6)\n",
    "        # optimize on alpha S_neg(1 f(alpha, x_i, D, theta)\n",
    "        alpha_temp_pos = scipy.optimize.minimize(function_S, alpha_pos, args= (x, D, w, b, lambda_0, lambda_1, True), tol=1e-6)\n",
    "\n",
    "        alpha_opt_pos[i,:] = alpha_temp_pos.x\n",
    "        alpha_opt_neg[i,:] = alpha_temp_neg.x\n",
    "        alpha_pos = alpha_temp_pos.x\n",
    "        alpha_neg = alpha_temp_neg.x\n",
    "\n",
    "    return alpha_opt_neg, alpha_opt_pos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/q3/sl_3k64s2ldb0zk8pjw9qplc0000gn/T/ipykernel_88986/2022937913.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0malpha_opt_neg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha_opt_pos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msupervised_sparse_coding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha_0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msignals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/q3/sl_3k64s2ldb0zk8pjw9qplc0000gn/T/ipykernel_88986/4107993294.py\u001b[0m in \u001b[0;36msupervised_sparse_coding\u001b[0;34m(alpha, signals, D, w, b)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msignals\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;31m# optimize on alpha S_neg(-1 f(alpha, x_i, D, theta)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0malpha_temp_neg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunction_S\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha_neg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0;31m# optimize on alpha S_neg(1 f(alpha, x_i, D, theta)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0malpha_temp_pos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunction_S\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha_pos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ts/lib/python3.8/site-packages/scipy/optimize/_minimize.py\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[1;32m    689\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_minimize_cg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjac\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'bfgs'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 691\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_minimize_bfgs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjac\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    692\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'newton-cg'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    693\u001b[0m         res = _minimize_newtoncg(fun, x0, args, jac, hess, hessp, callback,\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ts/lib/python3.8/site-packages/scipy/optimize/_optimize.py\u001b[0m in \u001b[0;36m_minimize_bfgs\u001b[0;34m(fun, x0, args, jac, callback, gtol, norm, eps, maxiter, disp, return_all, finite_diff_rel_step, xrtol, **unknown_options)\u001b[0m\n\u001b[1;32m   1386\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1387\u001b[0m             \u001b[0malpha_k\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mold_fval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mold_old_fval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgfkp1\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1388\u001b[0;31m                      _line_search_wolfe12(f, myfprime, xk, pk, gfk,\n\u001b[0m\u001b[1;32m   1389\u001b[0m                                           old_fval, old_old_fval, amin=1e-100, amax=1e100)\n\u001b[1;32m   1390\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0m_LineSearchError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ts/lib/python3.8/site-packages/scipy/optimize/_optimize.py\u001b[0m in \u001b[0;36m_line_search_wolfe12\u001b[0;34m(f, fprime, xk, pk, gfk, old_fval, old_old_fval, **kwargs)\u001b[0m\n\u001b[1;32m   1158\u001b[0m     \u001b[0mextra_condition\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'extra_condition'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1160\u001b[0;31m     ret = line_search_wolfe1(f, fprime, xk, pk, gfk,\n\u001b[0m\u001b[1;32m   1161\u001b[0m                              \u001b[0mold_fval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mold_old_fval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1162\u001b[0m                              **kwargs)\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ts/lib/python3.8/site-packages/scipy/optimize/_linesearch.py\u001b[0m in \u001b[0;36mline_search_wolfe1\u001b[0;34m(f, fprime, xk, pk, gfk, old_fval, old_old_fval, args, c1, c2, amax, amin, xtol)\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0mderphi0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgfk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m     stp, fval, old_fval = scalar_search_wolfe1(\n\u001b[0m\u001b[1;32m     85\u001b[0m             \u001b[0mphi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mderphi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mold_fval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mold_old_fval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mderphi0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m             c1=c1, c2=c2, amax=amax, amin=amin, xtol=xtol)\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ts/lib/python3.8/site-packages/scipy/optimize/_linesearch.py\u001b[0m in \u001b[0;36mscalar_search_wolfe1\u001b[0;34m(phi, derphi, phi0, old_phi0, derphi0, c1, c2, amax, amin, xtol)\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34mb'FG'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m             \u001b[0malpha1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m             \u001b[0mphi1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mphi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m             \u001b[0mderphi1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mderphi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ts/lib/python3.8/site-packages/scipy/optimize/_linesearch.py\u001b[0m in \u001b[0;36mphi\u001b[0;34m(s)\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mphi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0mfc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxk\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mderphi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ts/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\u001b[0m in \u001b[0;36mfun\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray_equal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_x_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ts/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\u001b[0m in \u001b[0;36mupdate_x\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    241\u001b[0m                 \u001b[0;31m# ensure that self.x is a copy of x. Don't store a reference\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m                 \u001b[0;31m# otherwise the memoization doesn't work properly.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matleast_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    244\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_updated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mg_updated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ts/lib/python3.8/site-packages/numpy/core/overrides.py\u001b[0m in \u001b[0;36matleast_1d\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ts/lib/python3.8/site-packages/numpy/core/shape_base.py\u001b[0m in \u001b[0;36m_atleast_1d_dispatcher\u001b[0;34m(*arys)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0m_atleast_1d_dispatcher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0marys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "alpha_opt_neg, alpha_opt_pos = supervised_sparse_coding(alpha_0, signals, D, w, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([np.linalg.norm(alpha_opt_pos[i] - alpha_opt_pos[-1], ord=1) for i in range(alpha_opt_pos.shape[0])])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradC(x):\n",
    "    # gradient of the logistic loss\n",
    "    return -1/(1+np.exp(x))\n",
    "\n",
    "def compute_gradients(D, w, b, X, Y, alpha, lambda_0, mu):\n",
    "    \"\"\"Compute gradients of E with respect to D, w, and b.\"\"\"\n",
    "    # Initialize gradients\n",
    "    grad_D = np.zeros_like(D)\n",
    "    grad_w = np.zeros_like(w)\n",
    "    grad_b = 0.0\n",
    "\n",
    "    # Iterate over data samples\n",
    "    for i in range(len(X)):\n",
    "        for z in [-1, 1]: #0 = class -1, 1 = class 1\n",
    "            # Compute residual and coefficients\n",
    "            if z==-1:\n",
    "                p = 0\n",
    "            else:\n",
    "                p = 1\n",
    "            alpha_i_z = alpha[:, i, p]  # Sparse code\n",
    "            residual = X[i] - D @ alpha_i_z\n",
    "\n",
    "            # Compute omega_i_z (gradient of C with respect to w^T alpha + b)\n",
    "            wx_plus_b = w.T @ alpha_i_z + b\n",
    "            diff_S = function_S(alpha[:, i, 0], X[i], D, w, b, lambda_0, pos=(Y[i]))\n",
    "            grad_C = (-1 / (1 + np.exp((z * wx_plus_b)))) * z\n",
    "\n",
    "            omega_i_z = -mu * grad_C\n",
    "\n",
    "            # Gradients\n",
    "            grad_D -= 2 * lambda_0 * omega_i_z * np.outer(residual, alpha_i_z)\n",
    "            grad_w += omega_i_z * alpha_i_z\n",
    "            grad_b += omega_i_z\n",
    "\n",
    "        return grad_D, grad_w, grad_b\n",
    "\n",
    "def project_D(D):\n",
    "    \"\"\"Project dictionary D to satisfy column constraints (e.g., unit-norm).\"\"\"\n",
    "    return D / np.maximum(np.linalg.norm(D, axis=0, keepdims=True), 1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def projected_gradient_descent(D, w, b, signals, alpha, lambda_0, mu, grad_steps=100, step_size=0.01, tol=1e-6):\n",
    "    \"\"\"\n",
    "    Projected Gradient Descent for optimizing E(D, theta) with dictionary D and linear parameters w, b.\n",
    "\n",
    "    Parameters:\n",
    "        D (ndarray): Initial dictionary, shape (d, k)\n",
    "        w (ndarray): Initial linear weights, shape (k,)\n",
    "        b (float): Initial bias term\n",
    "        X (ndarray): Input data matrix, shape (m, d)\n",
    "        alpha (ndarray): Sparse codes, shape (k, m, 2) for each z={-1,+1}\n",
    "        lambda_0 (float): Regularization parameter\n",
    "        mu (float): Classification tradeoff parameter\n",
    "        grad_steps (int): Number of gradient descent iterations\n",
    "        step_size (float): Step size for updates\n",
    "        tol (float): Tolerance for convergence\n",
    "\n",
    "    Returns:\n",
    "        D, w, b: Updated parameters\n",
    "    \"\"\"\n",
    "\n",
    "    m, d = signals.shape  # m samples, d dimensions\n",
    "    k = D.shape[1]  # k dictionary atoms\n",
    "\n",
    "    def compute_gradients(D, w, b, signals, alpha, lambda_0, mu):\n",
    "        \"\"\"Compute gradients of E with respect to D, w, and b.\"\"\"\n",
    "        # Initialize gradients\n",
    "        grad_D = np.zeros_like(D)\n",
    "        grad_w = np.zeros_like(w)\n",
    "        grad_b = 0.0\n",
    "\n",
    "        # Iterate over data samples\n",
    "        for i in range(m):\n",
    "            for z in [-1, +1]:\n",
    "                # Compute residual and coefficients\n",
    "                alpha_i_z = alpha[:, i, z]\n",
    "                residual = signals[i] - D @ alpha_i_z\n",
    "\n",
    "                # Compute omega_i_z\n",
    "                grad_C = (w.T @ alpha_i_z + b)  # Assume a placeholder gradient of C\n",
    "                omega_i_z = -mu * z * grad_C\n",
    "\n",
    "                # Gradients\n",
    "                grad_D -= 2 * lambda_0 * omega_i_z * np.outer(residual, alpha_i_z)\n",
    "                grad_w += omega_i_z * z * grad_C * alpha_i_z\n",
    "                grad_b += omega_i_z * z * grad_C\n",
    "\n",
    "        return grad_D, grad_w, grad_b\n",
    "\n",
    "\n",
    "    # Gradient Descent Loop\n",
    "    for step in range(grad_steps):\n",
    "        # Compute gradients\n",
    "        grad_D, grad_w, grad_b = compute_gradients(D, w, b, X, alpha, lambda_0, mu)\n",
    "\n",
    "        # Update parameters\n",
    "        D -= step_size * grad_D\n",
    "        w -= step_size * grad_w\n",
    "        b -= step_size * grad_b\n",
    "\n",
    "        # Project dictionary D\n",
    "        D = project_D(D)\n",
    "\n",
    "        # Convergence Check (based on gradient norms)\n",
    "        if np.linalg.norm(grad_D) < tol and np.linalg.norm(grad_w) < tol and abs(grad_b) < tol:\n",
    "            print(f\"Converged at step {step + 1}\")\n",
    "            break\n",
    "\n",
    "    return D, w, b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from scipy.optimize import minimize\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "class SupervisedDictionaryLearning(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, n_components=10, lambda0=0.1, lambda1=0.1, lambda2=0.1, max_iter=100, tol=1e-4, n_jobs=-1):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        - n_components: int, size of the dictionary (k)\n",
    "        - lambda0, lambda1, lambda2: float, regularization parameters\n",
    "        - max_iter: int, maximum number of iterations\n",
    "        - tol: float, convergence tolerance\n",
    "        - n_jobs: int, number of parallel jobs to run for sparse coding (default is -1 for all CPUs)\n",
    "        \"\"\"\n",
    "        self.n_components = n_components\n",
    "        self.lambda0 = lambda0\n",
    "        self.lambda1 = lambda1\n",
    "        self.lambda2 = lambda2\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "        self.n_jobs = n_jobs\n",
    "\n",
    "\n",
    "    def _initialize(self, X):\n",
    "        n_features = X.shape[1]\n",
    "        self.D_ = np.random.randn(n_features, self.n_components)\n",
    "        self.D_ /= np.linalg.norm(self.D_, axis=0)  # Normalize columns\n",
    "        self.theta_ = np.zeros(self.n_components + 1)  # For linear model (w, b)\n",
    "\n",
    "    def _sparse_coding(self, X):\n",
    "        \"\"\"Solve supervised sparse coding for each sample in parallel.\"\"\"\n",
    "        m = X.shape[0]\n",
    "        # Parallelize the computation of alpha for each sample\n",
    "        alpha_neg = Parallel(n_jobs=self.n_jobs)(delayed(self._solve_alpha)(X[i], -1) for i in range(m))\n",
    "        alpha_pos = Parallel(n_jobs=self.n_jobs)(delayed(self._solve_alpha)(X[i], 1) for i in range(m))\n",
    "        return np.array(alpha_neg), np.array(alpha_pos)\n",
    "\n",
    "    def _solve_alpha(self, xi, label):\n",
    "        \"\"\"Solve the sparse coding problem for a single sample.\"\"\"\n",
    "        def objective(alpha):\n",
    "            reconstruction_error = np.linalg.norm(xi - self.D_ @ alpha)**2\n",
    "            sparsity_penalty = self.lambda1 * np.linalg.norm(alpha, 1)\n",
    "            classification_loss = np.log(1 + np.exp(-label * (self.theta_[:-1] @ alpha + self.theta_[-1])))\n",
    "            return classification_loss + self.lambda0 * reconstruction_error + sparsity_penalty\n",
    "\n",
    "        result = minimize(objective, np.zeros(self.n_components), method='L-BFGS-B')\n",
    "        return result.x\n",
    "\n",
    "    def _update_dictionary_and_params(self, X, y, alpha_neg, alpha_pos, lr_D=0.01, lr_theta=0.01):\n",
    "        \"\"\"Update D and theta using provided gradients.\"\"\"\n",
    "        m = X.shape[0]\n",
    "        self.list_mu = 0*np.linspace(0, 1, m)\n",
    "        grad_D = np.zeros_like(self.D_)\n",
    "        grad_theta_w = np.zeros(self.n_components)\n",
    "        grad_theta_b = 0\n",
    "\n",
    "        for i in range(m):\n",
    "            xi = X[i]\n",
    "            yi = y[i]\n",
    "\n",
    "            # Compute omega\n",
    "            S_neg = self._compute_S(alpha_neg[i], xi, yi, -1)\n",
    "            S_pos = self._compute_S(alpha_pos[i], xi, yi, +1)\n",
    "            omega_neg = - self.list_mu[i] * (-1) * self._nabla_C(S_neg - S_pos) + (1 - self.list_mu[i]) * int(yi == -1)\n",
    "            omega_pos = -self.list_mu[i] * (+1) * self._nabla_C(S_neg - S_pos) + (1 - self.list_mu[i]) * int(yi == +1)\n",
    "\n",
    "            # Update gradients for D\n",
    "            grad_D += omega_neg * np.outer((xi - self.D_ @ alpha_neg[i]), alpha_neg[i])\n",
    "            grad_D += omega_pos * np.outer((xi - self.D_ @ alpha_pos[i]), alpha_pos[i])\n",
    "\n",
    "            # Update gradients for theta (w and b)\n",
    "            pred_neg = self.theta_[:-1] @ alpha_neg[i] + self.theta_[-1]\n",
    "            pred_pos = self.theta_[:-1] @ alpha_pos[i] + self.theta_[-1]\n",
    "            grad_theta_w += omega_neg * (-1) * self._nabla_C(pred_neg) * alpha_neg[i]\n",
    "            grad_theta_w += omega_pos * (+1) * self._nabla_C(pred_pos) * alpha_pos[i]\n",
    "            grad_theta_b += omega_neg * (-1) * self._nabla_C(pred_neg)\n",
    "            grad_theta_b += omega_pos * (+1) * self._nabla_C(pred_pos)\n",
    "\n",
    "        grad_D *= - 2 * self.lambda0\n",
    "        # Gradient updates\n",
    "        self.D_ -= lr_D * grad_D  # Learning rate for D\n",
    "        self.D_ /= np.linalg.norm(self.D_, axis=0)  # Re-normalize columns\n",
    "        self.theta_[:-1] -= lr_theta * grad_theta_w  # Learning rate for w\n",
    "        self.theta_[-1] -= lr_theta * grad_theta_b  # Learning rate for b\n",
    "\n",
    "    def _compute_S(self, alpha, xi, yi, label):\n",
    "        \"\"\"Compute the loss S for a given alpha.\"\"\"\n",
    "        reconstruction_error = np.linalg.norm(xi - self.D_ @ alpha)**2\n",
    "        sparsity_penalty = self.lambda1 * np.linalg.norm(alpha, 1)\n",
    "        classification_loss = np.log(1 + np.exp(-label * (self.theta_[:-1] @ alpha + self.theta_[-1])))\n",
    "        return classification_loss + self.lambda0 * reconstruction_error + sparsity_penalty\n",
    "\n",
    "    def _nabla_C(self, x):\n",
    "        \"\"\"Gradient of the logistic loss.\"\"\"\n",
    "        return -1 / (1 + np.exp(x))\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit the model to the data.\"\"\"\n",
    "        self._initialize(X)\n",
    "        for iteration in range(self.max_iter):\n",
    "            alpha_neg, alpha_pos = self._sparse_coding(X)\n",
    "            self._update_dictionary_and_params(X, y, alpha_neg, alpha_pos)\n",
    "\n",
    "            # Log progress\n",
    "            current_loss = np.mean([self._compute_S(alpha_neg[i], X[i], y[i], -1) +\n",
    "                                    self._compute_S(alpha_pos[i], X[i], y[i], +1) for i in range(len(y))])\n",
    "            print(f\"Iteration {iteration + 1}/{self.max_iter}, Loss: {current_loss:.4f}\")\n",
    "\n",
    "            # Check convergence\n",
    "            if iteration > 0 and np.linalg.norm(alpha_neg - alpha_pos) < self.tol:\n",
    "                print(\"Convergence reached.\")\n",
    "                break\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict class labels for samples in X.\"\"\"\n",
    "        alpha_list = np.array([self._solve_alpha(xi, 1) for xi in X])  # Assume yi=1 for prediction\n",
    "        scores = alpha_list @ self.theta_[:-1] + self.theta_[-1]\n",
    "        return np.sign(scores)\n",
    "\n",
    "    def score(self, X, y):\n",
    "        \"\"\"Return the classification accuracy.\"\"\"\n",
    "        y_pred = self.predict(X)\n",
    "        return np.mean(y_pred == y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SyntheticTimeSeriesDataset:\n",
    "    def __init__(self, num_classes=3, num_samples_per_class=100,\n",
    "                 sequence_length=100):\n",
    "        self.num_classes = num_classes\n",
    "        self.num_samples_per_class = num_samples_per_class\n",
    "        self.sequence_length = sequence_length\n",
    "        self.data = None\n",
    "        self.labels = None\n",
    "\n",
    "    def generate_class_data(self, class_id):\n",
    "        np.random.seed(class_id)\n",
    "        data, labels = [], []\n",
    "        time = np.linspace(0, 2 * np.pi, self.sequence_length)\n",
    "\n",
    "        for _ in range(self.num_samples_per_class):\n",
    "            if class_id == 0:\n",
    "                signal = (\n",
    "                    np.sin((class_id + 1) * time) +\n",
    "                    0.1 * np.random.randn(self.sequence_length)\n",
    "                )\n",
    "            elif class_id == 1:\n",
    "                signal = (\n",
    "                    np.cos((class_id + 1) * time) +\n",
    "                    0.1 * np.random.randn(self.sequence_length)\n",
    "                )\n",
    "            elif class_id == 2:\n",
    "                signal = (\n",
    "                    np.sin((class_id + 1) * time) *\n",
    "                    np.exp(-0.05 * time) +\n",
    "                    0.1 * np.random.randn(self.sequence_length)\n",
    "                )\n",
    "            data.append(signal)\n",
    "            labels.append(class_id)\n",
    "        return np.array(data), np.array(labels)\n",
    "\n",
    "    def create_dataset(self):\n",
    "        data, labels = [], []\n",
    "        for class_id in range(self.num_classes):\n",
    "            class_data, class_labels = self.generate_class_data(class_id)\n",
    "            data.append(class_data)\n",
    "            labels.append(class_labels)\n",
    "        self.data, self.labels = np.vstack(data), np.hstack(labels)\n",
    "        return self.data, self.labels  # Explicitly return X and y\n",
    "\n",
    "\n",
    "\n",
    "    def plot_examples(self):\n",
    "        if self.data is None or self.labels is None:\n",
    "            raise ValueError(\"Dataset has not been created. \"\n",
    "                             \"Call create_dataset() first.\")\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        for class_id in range(self.num_classes):\n",
    "            idx = class_id * self.num_samples_per_class\n",
    "            plt.plot(self.data[idx], label=f\"Class {class_id}\")\n",
    "        plt.title(\"Example Time Series for Each Class\")\n",
    "        plt.xlabel(\"Time Step\")\n",
    "        plt.ylabel(\"Value\")\n",
    "        plt.legend()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1/1, Loss: 57.9779\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "_solve_alpha() takes 3 positional arguments but 4 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/q3/sl_3k64s2ldb0zk8pjw9qplc0000gn/T/ipykernel_88986/1158656590.py\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# Evaluate the model on the test set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Test set accuracy:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/q3/sl_3k64s2ldb0zk8pjw9qplc0000gn/T/ipykernel_88986/675239142.py\u001b[0m in \u001b[0;36mscore\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    123\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0;34m\"\"\"Return the classification accuracy.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/q3/sl_3k64s2ldb0zk8pjw9qplc0000gn/T/ipykernel_88986/675239142.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;34m\"\"\"Predict class labels for samples in X.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m         \u001b[0malpha_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_solve_alpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mxi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Assume yi=1 for prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malpha_list\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtheta_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtheta_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/q3/sl_3k64s2ldb0zk8pjw9qplc0000gn/T/ipykernel_88986/675239142.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;34m\"\"\"Predict class labels for samples in X.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m         \u001b[0malpha_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_solve_alpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mxi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Assume yi=1 for prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malpha_list\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtheta_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtheta_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: _solve_alpha() takes 3 positional arguments but 4 were given"
     ]
    }
   ],
   "source": [
    "# Generate the entire dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "dataset = SyntheticTimeSeriesDataset(num_classes=2, num_samples_per_class=100, sequence_length=100)\n",
    "X, y = dataset.create_dataset()\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# Use the train set to fit the model\n",
    "# choice of parameters so the lambda1/lambda0 ratio is 0.15\n",
    "model = SupervisedDictionaryLearning(n_components=10, lambda0=1, lambda1=0.15, lambda2=0.2, max_iter=1)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "accuracy = model.score(X_test, y_test)\n",
    "print(\"Test set accuracy:\", accuracy)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ts",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

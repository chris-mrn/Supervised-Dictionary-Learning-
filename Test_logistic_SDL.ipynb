{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from Models.utils import PrimalDualSolver_logistic\n",
    "from Models.utils import ProjectedGradientDescent_logistic\n",
    "\n",
    "class SDL_logistic:\n",
    "    \"\"\"\n",
    "    Supervised Dictionary Learning (SDL) Class.\n",
    "\n",
    "    Combines sparse coding with supervised learning by jointly learning:\n",
    "    - A dictionary `D` for sparse representation.\n",
    "    - A linear model `(theta, b)` for predicting labels.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_iter=1000,\n",
    "                 lamnda0=0.01,\n",
    "                 lambda1=0.01,\n",
    "                 lambda2=0.01,\n",
    "                 lr_D=0.01,\n",
    "                 lr_theta=0.01,\n",
    "                 lr_alpha=0.01):\n",
    "        self.n_iter = n_iter\n",
    "        self.lamnda0 = lamnda0\n",
    "        self.lambda1 = lambda1\n",
    "        self.lambda2 = lambda2\n",
    "        self.lr_D = lr_D\n",
    "        self.lr_theta = lr_theta\n",
    "        self.lr_alpha = lr_alpha\n",
    "\n",
    "    def objective(self, X, y, D, theta, b, alpha):\n",
    "        \"\"\"Computes the objective function value.\"\"\"\n",
    "        objective = 0\n",
    "        for i in range(X.shape[0]):\n",
    "            xi, yi, ai = X[i], y[i], alpha[i]\n",
    "            loss_dict = np.linalg.norm(xi - D @ ai)**2\n",
    "            z = yi * (theta @ ai + b)\n",
    "            loss_class = np.log(1 + np.exp(-z))\n",
    "            sparse_penalty = self.lambda1 * np.linalg.norm(ai, 1)\n",
    "            objective += loss_dict + loss_class + sparse_penalty\n",
    "        return objective\n",
    "\n",
    "    def solve_alpha(self, X, y, D, theta, b):\n",
    "        \"\"\"\n",
    "        Optimizes sparse codes `alpha` for fixed `D` and `theta`.\n",
    "        We can have here a explicit expression of our gradient regarding\n",
    "        to alpha, so we can use a proximal gradient descent to optimize it.\n",
    "        \"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "        alpha = np.zeros((n_samples, n_features))\n",
    "\n",
    "        for i in range(n_samples):\n",
    "            x_i = X[i]\n",
    "            y_i = y[i]\n",
    "\n",
    "            solver = PrimalDualSolver_logistic(\n",
    "                theta=theta, b=b, x_i=x_i, y_i=y_i, D=D,\n",
    "                lambda_0=self.lamnda0, lambda_1=self.lambda1,\n",
    "                lambd=0.0001, mu=1.0\n",
    "            )\n",
    "            # Solve the problem\n",
    "            x0 = np.random.randn(n_features)  # Random initialization\n",
    "            alpha_opt, _ = solver.solve(x0)\n",
    "\n",
    "            alpha[i] = alpha_opt\n",
    "\n",
    "        return alpha\n",
    "\n",
    "    def solve_D_theta(self, alpha_opt, X, y, D_opt, theta_opt, b):\n",
    "        \"\"\"\n",
    "        Updates `D` and `theta` given the optimal `alpha`.\n",
    "        Do a projective gradient descent.\n",
    "        \"\"\"\n",
    "        pgd = ProjectedGradientDescent_logistic(\n",
    "            D_init=D_opt, theta_init=theta_opt,\n",
    "            b=b, x=X, y=y, alphas=alpha_opt,\n",
    "            lambda_0=self.lamnda0,\n",
    "            lambda_1=self.lambda1, lambda_2=self.lambda2,\n",
    "            lr=self.lr_D, max_iter=self.n_iter\n",
    "        )\n",
    "        D_opt, theta_opt, b_opt, _ = pgd.optimize()\n",
    "        return D_opt, theta_opt, b_opt\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fits the model to the data.\"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "        self.n_components = n_features\n",
    "        D_opt = np.random.randn(n_features, self.n_components)\n",
    "        D_opt /= np.linalg.norm(D_opt, axis=0)\n",
    "        theta_opt = np.zeros(self.n_components)\n",
    "        b_opt = 0\n",
    "\n",
    "        for i in range(self.n_iter):\n",
    "            alpha_opt = self.solve_alpha(X, y, D_opt, theta_opt, b_opt)\n",
    "            D_opt, theta_opt, b_opt = self.solve_D_theta(alpha_opt,\n",
    "                                                         X,\n",
    "                                                         y,\n",
    "                                                         D_opt,\n",
    "                                                         theta_opt,\n",
    "                                                         b_opt)\n",
    "            # Print the loss of after each iteration\n",
    "            print(f\"Iteration {i+1}/{self.n_iter}, Loss: {self.objective(X, y, D_opt, theta_opt, b_opt, alpha_opt)}\")\n",
    "\n",
    "\n",
    "        self.alpha = alpha_opt\n",
    "        self.D = D_opt\n",
    "        self.theta = theta_opt\n",
    "        self.b = b_opt\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predicts labels for input data `X`.\"\"\"\n",
    "        return self.theta @ self.alpha.T + self.b\n",
    "\n",
    "    def score(self, X, y):\n",
    "        \"\"\"Computes classification accuracy.\"\"\"\n",
    "        y_pred = self.predict(X)\n",
    "        return np.mean(np.round(y_pred) == y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (100, 40)\n",
      "Shape of y: (100,)\n",
      "First 10 labels: [-1.  1. -1. -1. -1. -1. -1.  1.  1.  1.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Parameters\n",
    "n_samples = 100  # Number of samples\n",
    "n_features = 40  # Number of features\n",
    "n_components = 10  # Number of latent components\n",
    "noise_level = 0.1  # Noise level in the data\n",
    "\n",
    "# Generate synthetic data\n",
    "X = np.random.randn(n_samples, n_features)  # Input data (features)\n",
    "\n",
    "# True parameters for the dictionary learning model\n",
    "true_D = np.random.randn(n_features, n_components)  # True dictionary\n",
    "true_alpha = np.random.randn(n_samples, n_components)  # True sparse codes\n",
    "true_theta = np.random.randn(n_components)  # True linear model weights\n",
    "b_true = 0.5  # Bias term\n",
    "\n",
    "# Generate continuous labels based on a linear model\n",
    "y_continuous = X @ true_D @ true_theta + b_true + noise_level * np.random.randn(n_samples)\n",
    "\n",
    "# Convert continuous labels into binary labels (-1 or 1)\n",
    "y = np.sign(y_continuous)  # Use sign function to classify labels as -1 or 1\n",
    "\n",
    "# Display results\n",
    "print(\"Shape of X:\", X.shape)\n",
    "print(\"Shape of y:\", y.shape)\n",
    "print(\"First 10 labels:\", y[:10])\n",
    "\n",
    "\n",
    "# Initialize the SDL model\n",
    "sdl = SDL_logistic(\n",
    "        n_iter=10,\n",
    "        lamnda0=1,\n",
    "        lambda1=0.15,\n",
    "        lambda2=1,\n",
    "        lr_D=0.001,\n",
    "        lr_theta=0.001,\n",
    "        lr_alpha=0.001\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1/10, Loss: 3099.0619809735585\n",
      "Iteration 2/10, Loss: 3097.168915982063\n",
      "Iteration 3/10, Loss: 3143.1430254231736\n",
      "Iteration 4/10, Loss: 3131.8407197226425\n",
      "Iteration 5/10, Loss: 3085.6781693188823\n",
      "Iteration 6/10, Loss: 3172.4034755620155\n",
      "Iteration 7/10, Loss: 3166.7777784785403\n",
      "Iteration 8/10, Loss: 3130.261731129302\n"
     ]
    }
   ],
   "source": [
    "# do a train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "\n",
    "# Fit the model to the data\n",
    "sdl.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "train_accuracy = sdl.score(X_train, y_train)\n",
    "test_accuracy = sdl.score(X_test, np.round(y_test))\n",
    "print(f\"Train accuracy: {train_accuracy:.2f}\")\n",
    "print(f\"Test accuracy: {test_accuracy:.2f}\")\n",
    "\n",
    "print(y_test)\n",
    "\n",
    "# print predicted labels\n",
    "print(np.round(sdl.predict(X_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4.\n",
      " 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4.\n",
      " 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4.]\n"
     ]
    }
   ],
   "source": [
    "print(np.round(sdl.predict(X_test)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ts",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

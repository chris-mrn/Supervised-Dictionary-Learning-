{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append('../Models')\n",
    "from sklearn.model_selection import train_test_split\n",
    "from utils import PrimalDualSolver_logistic\n",
    "from utils import ProjectedGradientDescent_logistic\n",
    "\n",
    "class SDL_logistic:\n",
    "    \"\"\"\n",
    "    Supervised Dictionary Learning (SDL) Class.\n",
    "\n",
    "    Combines sparse coding with supervised learning by jointly learning:\n",
    "    - A dictionary `D` for sparse representation.\n",
    "    - A linear model `(theta, b)` for predicting labels.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_iter=1000,\n",
    "                 lamnda0=0.01,\n",
    "                 lambda1=0.01,\n",
    "                 lambda2=0.01,\n",
    "                 lr_D=0.01,\n",
    "                 lr_theta=0.01,\n",
    "                 lr_alpha=0.01):\n",
    "        self.n_iter = n_iter\n",
    "        self.lamnda0 = lamnda0\n",
    "        self.lambda1 = lambda1\n",
    "        self.lambda2 = lambda2\n",
    "        self.lr_D = lr_D\n",
    "        self.lr_theta = lr_theta\n",
    "        self.lr_alpha = lr_alpha\n",
    "\n",
    "    def objective(self, X, y, D, theta, b, alpha):\n",
    "        \"\"\"Computes the objective function value.\"\"\"\n",
    "        objective = 0\n",
    "        for i in range(X.shape[0]):\n",
    "            xi, yi, ai = X[i], y[i], alpha[i]\n",
    "            loss_dict = np.linalg.norm(xi - D @ ai)**2\n",
    "            z = yi * (theta @ ai + b)\n",
    "            loss_class = np.log(1 + np.exp(-z))\n",
    "            sparse_penalty = self.lambda1 * np.linalg.norm(ai, 1)\n",
    "            objective += loss_dict + loss_class + sparse_penalty\n",
    "        return objective\n",
    "\n",
    "    def loss_prediction(self, X, y, D, theta, b, alpha):\n",
    "        \"\"\"Computes the classification loss.\"\"\"\n",
    "        loss = 0\n",
    "        for i in range(X.shape[0]):\n",
    "            xi, yi, ai = X[i], y[i], alpha[i]\n",
    "            z = yi * (theta @ ai + b)\n",
    "            loss += np.log(1 + np.exp(-z))\n",
    "        return loss\n",
    "\n",
    "    def solve_alpha(self, X, y, D, theta, b):\n",
    "        \"\"\"\n",
    "        Optimizes sparse codes `alpha` for fixed `D` and `theta`.\n",
    "        We can have here a explicit expression of our gradient regarding\n",
    "        to alpha, so we can use a proximal gradient descent to optimize it.\n",
    "        \"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "        alpha = np.zeros((n_samples, n_features))\n",
    "\n",
    "        for i in range(n_samples):\n",
    "            x_i = X[i]\n",
    "            y_i = y[i]\n",
    "\n",
    "            solver = PrimalDualSolver_logistic(\n",
    "                theta=theta, b=b, x_i=x_i, y_i=y_i, D=D,\n",
    "                lambda_0=self.lamnda0, lambda_1=self.lambda1,\n",
    "                lambd=0.01, mu=1.0\n",
    "            )\n",
    "            # Solve the problem\n",
    "            x0 = np.random.randn(n_features)  # Random initialization\n",
    "            alpha_opt, _ = solver.solve(x0)\n",
    "\n",
    "            alpha[i] = alpha_opt\n",
    "\n",
    "        return alpha\n",
    "\n",
    "    def solve_D_theta(self, alpha_opt, X, y, D_opt, theta_opt, b):\n",
    "        \"\"\"\n",
    "        Updates `D` and `theta` given the optimal `alpha`.\n",
    "        Do a projective gradient descent.\n",
    "        \"\"\"\n",
    "        pgd = ProjectedGradientDescent_logistic(\n",
    "            D_init=D_opt, theta_init=theta_opt,\n",
    "            b=b, x=X, y=y, alphas=alpha_opt,\n",
    "            lambda_0=self.lamnda0,\n",
    "            lambda_1=self.lambda1, lambda_2=self.lambda2,\n",
    "            lr=self.lr_D, max_iter=self.n_iter\n",
    "        )\n",
    "        D_opt, theta_opt, b_opt, _ = pgd.optimize()\n",
    "        return D_opt, theta_opt, b_opt\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fits the model to the data.\"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "        self.n_components = n_features\n",
    "        D_opt = np.random.randn(n_features, self.n_components)\n",
    "        D_opt /= np.linalg.norm(D_opt, axis=0)\n",
    "        theta_opt = np.zeros(self.n_components)\n",
    "        b_opt = 0\n",
    "\n",
    "        for i in range(self.n_iter):\n",
    "            alpha_opt = self.solve_alpha(X, y, D_opt, theta_opt, b_opt)\n",
    "            D_opt, theta_opt, b_opt = self.solve_D_theta(alpha_opt,\n",
    "                                                         X,\n",
    "                                                         y,\n",
    "                                                         D_opt,\n",
    "                                                         theta_opt,\n",
    "                                                         b_opt)\n",
    "            # Print the loss of after each iteration\n",
    "            print(f\"Iteration {i+1}/{self.n_iter}, Loss: {self.objective(X, y, D_opt, theta_opt, b_opt, alpha_opt)}\")\n",
    "            print(f\"Iteration {i+1}/{self.n_iter}, Loss classification: {self.loss_prediction(X, y, D_opt, theta_opt, b_opt, alpha_opt)}\")\n",
    "\n",
    "\n",
    "\n",
    "        self.alpha = alpha_opt\n",
    "        self.D = D_opt\n",
    "        self.theta = theta_opt\n",
    "        self.b = b_opt\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predicts labels for input data `X`.\"\"\"\n",
    "        predictions = []\n",
    "        for i in range(X.shape[0]):\n",
    "            x_i = X[i]\n",
    "            alpha, _, _, _ = np.linalg.lstsq(self.D, x_i, rcond=None)\n",
    "            prediction = self.theta @ alpha + self.b\n",
    "            predictions.append(prediction)\n",
    "        return np.array(predictions)\n",
    "\n",
    "    def score(self, X, y):\n",
    "        \"\"\"Computes classification accuracy.\"\"\"\n",
    "        y_pred = self.predict(X)\n",
    "        return np.mean(np.round(y_pred)== y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Parameters\n",
    "n_samples = 100  # Number of samples\n",
    "n_features = 40  # Number of features\n",
    "n_components = 10  # Number of latent components\n",
    "noise_level = 0.1  # Noise level in the data\n",
    "\n",
    "# Generate synthetic data\n",
    "X = np.random.randn(n_samples, n_features)  # Input data (features)\n",
    "\n",
    "# True parameters for the dictionary learning model\n",
    "true_D = np.random.randn(n_features, n_components)  # True dictionary\n",
    "true_alpha = np.random.randn(n_samples, n_components)  # True sparse codes\n",
    "true_theta = np.random.randn(n_components)  # True linear model weights\n",
    "b_true = 0.5  # Bias term\n",
    "\n",
    "# Generate continuous labels based on a linear model\n",
    "y_continuous = X @ true_D @ true_theta + b_true + noise_level * np.random.randn(n_samples)\n",
    "\n",
    "# Convert continuous labels into binary labels (-1 or 1)\n",
    "y = np.sign(y_continuous)  # Use sign function to classify labels as -1 or 1\n",
    "\n",
    "# Initialize the SDL model\n",
    "sdl = SDL_logistic(\n",
    "        n_iter=10,\n",
    "        lamnda0=0.1,\n",
    "        lambda1=0.1,\n",
    "        lambda2=0.1,\n",
    "        lr_D=0.01,\n",
    "        lr_theta=0.01,\n",
    "        lr_alpha=0.01\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1/10, Loss: 2847.956770260508\n",
      "Iteration 1/10, Loss classification: 29.441661400957017\n",
      "Iteration 2/10, Loss: 2743.4769783327997\n",
      "Iteration 2/10, Loss classification: 5.857496647776601\n",
      "Iteration 3/10, Loss: 2744.6397172239185\n",
      "Iteration 3/10, Loss classification: 4.380853456365288\n",
      "Iteration 4/10, Loss: 2742.329543655911\n",
      "Iteration 4/10, Loss classification: 4.009973465661632\n",
      "Iteration 5/10, Loss: 2738.5519163820672\n",
      "Iteration 5/10, Loss classification: 3.766460354168383\n",
      "Iteration 6/10, Loss: 2765.4029238376706\n",
      "Iteration 6/10, Loss classification: 3.5918458912178375\n",
      "Iteration 7/10, Loss: 2761.0306311275954\n",
      "Iteration 7/10, Loss classification: 3.642785354874404\n",
      "Iteration 8/10, Loss: 2740.453909291229\n",
      "Iteration 8/10, Loss classification: 3.3090631753033795\n",
      "Iteration 9/10, Loss: 2739.5244316250037\n",
      "Iteration 9/10, Loss classification: 2.8635275006365197\n",
      "Iteration 10/10, Loss: 2740.7073946628498\n",
      "Iteration 10/10, Loss classification: 2.9952595262550346\n",
      "Train accuracy: 0.05714285714285714\n",
      "Test accuracy: 0.03333333333333333\n",
      "Ground truth labels: [ 1. -1.  1. -1. -1. -1.  1.  1.  1.  1.]\n",
      "Predicted labels: [-0.  0. -0.  0. -0.  0.  0.  1.  0. -0.]\n",
      "Number of right predictions: 1\n"
     ]
    }
   ],
   "source": [
    "# do a train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "# Fit the model to the data\n",
    "sdl.fit(X_train, y_train)\n",
    "\n",
    "# do the prediction\n",
    "y_pred_train = sdl.predict(X_train)\n",
    "print(\"Train accuracy:\", np.mean(np.round(y_pred_train) == y_train))\n",
    "\n",
    "y_pred_test = sdl.predict(X_test)\n",
    "print(\"Test accuracy:\", np.mean(np.round(y_pred_test) == y_test))\n",
    "print(\"Ground truth labels:\", y_test[:10])\n",
    "print(\"Predicted labels:\", np.round(y_pred_test)[:10])\n",
    "\n",
    "# count the number of right predictions\n",
    "print(\"Number of right predictions:\", np.sum(np.round(y_pred_test) == y_test))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ts",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append('../Models')\n",
    "from sklearn.model_selection import train_test_split\n",
    "from utils import PrimalDualSolver_logistic\n",
    "from utils import ProjectedGradientDescent_logistic\n",
    "\n",
    "class SDL_logistic:\n",
    "    \"\"\"\n",
    "    Supervised Dictionary Learning (SDL) Class.\n",
    "\n",
    "    Combines sparse coding with supervised learning by jointly learning:\n",
    "    - A dictionary `D` for sparse representation.\n",
    "    - A linear model `(theta, b)` for predicting labels.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_iter=1000,\n",
    "                 lamnda0=0.01,\n",
    "                 lambda1=0.01,\n",
    "                 lambda2=0.01,\n",
    "                 lr_D=0.01,\n",
    "                 lr_theta=0.01,\n",
    "                 lr_alpha=0.01):\n",
    "        self.n_iter = n_iter\n",
    "        self.lamnda0 = lamnda0\n",
    "        self.lambda1 = lambda1\n",
    "        self.lambda2 = lambda2\n",
    "        self.lr_D = lr_D\n",
    "        self.lr_theta = lr_theta\n",
    "        self.lr_alpha = lr_alpha\n",
    "\n",
    "    def objective(self, X, y, D, theta, b, alpha):\n",
    "        \"\"\"Computes the objective function value.\"\"\"\n",
    "        objective = 0\n",
    "        for i in range(X.shape[0]):\n",
    "            xi, yi, ai = X[i], y[i], alpha[i]\n",
    "            loss_dict = np.linalg.norm(xi - D @ ai)**2\n",
    "            z = yi * (theta @ ai + b)\n",
    "            loss_class = np.log(1 + np.exp(-z))\n",
    "            sparse_penalty = self.lambda1 * np.linalg.norm(ai, 1)\n",
    "            objective += loss_dict + loss_class + sparse_penalty\n",
    "        return objective\n",
    "\n",
    "    def loss_prediction(self, X, y, D, theta, b, alpha):\n",
    "        \"\"\"Computes the classification loss.\"\"\"\n",
    "        loss = 0\n",
    "        for i in range(X.shape[0]):\n",
    "            xi, yi, ai = X[i], y[i], alpha[i]\n",
    "            z = yi * (theta @ ai + b)\n",
    "            loss += np.log(1 + np.exp(-z))\n",
    "        return loss\n",
    "\n",
    "    def solve_alpha(self, X, y, D, theta, b):\n",
    "        \"\"\"\n",
    "        Optimizes sparse codes `alpha` for fixed `D` and `theta`.\n",
    "        We can have here a explicit expression of our gradient regarding\n",
    "        to alpha, so we can use a proximal gradient descent to optimize it.\n",
    "        \"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "        alpha = np.zeros((n_samples, n_features))\n",
    "\n",
    "        for i in range(n_samples):\n",
    "            x_i = X[i]\n",
    "            y_i = y[i]\n",
    "\n",
    "            solver = PrimalDualSolver_logistic(\n",
    "                theta=theta, b=b, x_i=x_i, y_i=y_i, D=D,\n",
    "                lambda_0=self.lamnda0, lambda_1=self.lambda1,\n",
    "                lambd=0.01, mu=1.0\n",
    "            )\n",
    "            # Solve the problem\n",
    "            x0 = np.random.randn(n_features)  # Random initialization\n",
    "            alpha_opt, _ = solver.solve(x0)\n",
    "\n",
    "            alpha[i] = alpha_opt\n",
    "\n",
    "        return alpha\n",
    "\n",
    "    def solve_D_theta(self, alpha_opt, X, y, D_opt, theta_opt, b):\n",
    "        \"\"\"\n",
    "        Updates `D` and `theta` given the optimal `alpha`.\n",
    "        Do a projective gradient descent.\n",
    "        \"\"\"\n",
    "        pgd = ProjectedGradientDescent_logistic(\n",
    "            D_init=D_opt, theta_init=theta_opt,\n",
    "            b=b, x=X, y=y, alphas=alpha_opt,\n",
    "            lambda_0=self.lamnda0,\n",
    "            lambda_1=self.lambda1, lambda_2=self.lambda2,\n",
    "            lr=self.lr_D, max_iter=self.n_iter\n",
    "        )\n",
    "        D_opt, theta_opt, b_opt, _ = pgd.optimize()\n",
    "        return D_opt, theta_opt, b_opt\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fits the model to the data.\"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "        self.n_components = n_features\n",
    "        D_opt = np.random.randn(n_features, self.n_components)\n",
    "        D_opt /= np.linalg.norm(D_opt, axis=0)\n",
    "        theta_opt = np.zeros(self.n_components)\n",
    "        b_opt = 0\n",
    "\n",
    "        for i in range(self.n_iter):\n",
    "            alpha_opt = self.solve_alpha(X, y, D_opt, theta_opt, b_opt)\n",
    "            D_opt, theta_opt, b_opt = self.solve_D_theta(alpha_opt,\n",
    "                                                         X,\n",
    "                                                         y,\n",
    "                                                         D_opt,\n",
    "                                                         theta_opt,\n",
    "                                                         b_opt)\n",
    "            # Print the loss of after each iteration\n",
    "            print(f\"Iteration {i+1}/{self.n_iter}, Loss: {self.objective(X, y, D_opt, theta_opt, b_opt, alpha_opt)}\")\n",
    "            print(f\"Iteration {i+1}/{self.n_iter}, Loss classification: {self.loss_prediction(X, y, D_opt, theta_opt, b_opt, alpha_opt)}\")\n",
    "\n",
    "\n",
    "\n",
    "        self.alpha = alpha_opt\n",
    "        self.D = D_opt\n",
    "        self.theta = theta_opt\n",
    "        self.b = b_opt\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predicts labels for input data `X`.\"\"\"\n",
    "        predictions = []\n",
    "        for i in range(X.shape[0]):\n",
    "            x_i = X[i]\n",
    "            alpha, _, _, _ = np.linalg.lstsq(self.D, x_i, rcond=None)\n",
    "            prediction = self.theta @ alpha + self.b\n",
    "            predictions.append(prediction)\n",
    "        return np.sign(np.array(predictions))\n",
    "\n",
    "    def score(self, X, y):\n",
    "        \"\"\"Computes classification accuracy.\"\"\"\n",
    "        y_pred = self.predict(X)\n",
    "        return np.mean(np.round(y_pred)== y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create synthetic data with sin for class 1 and cos for class -1\n",
    "# do different scaling for the two classes\n",
    "n_samples = 1000\n",
    "n_features = 100\n",
    "n_classes = 2\n",
    "X = np.random.randn(n_samples, n_features)\n",
    "y = np.random.choice([-1, 1], n_samples)\n",
    "X[y == 1] = 2 * np.pi * X[y == 1]\n",
    "X[y == -1] = 2 * np.pi * X[y == -1] + np.pi\n",
    "X = np.sin(X)\n",
    "X = X / np.linalg.norm(X, axis=1)[:, None]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "\n",
    "# Initialize the SDL model\n",
    "sdl = SDL_logistic(\n",
    "        n_iter=40,\n",
    "        lamnda0=0.1,\n",
    "        lambda1=0.1,\n",
    "        lambda2=0.001,\n",
    "        lr_D=0.01,\n",
    "        lr_theta=0.01,\n",
    "        lr_alpha=0.01\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1/40, Loss: 1269.1370918374034\n",
      "Iteration 1/40, Loss classification: 469.98125953923267\n",
      "Iteration 2/40, Loss: 1991.6627763251734\n",
      "Iteration 2/40, Loss classification: 3.1043462920342457\n",
      "Iteration 3/40, Loss: 1933.614046497542\n",
      "Iteration 3/40, Loss classification: 5.947113334778715\n",
      "Iteration 4/40, Loss: 1901.8518186872316\n",
      "Iteration 4/40, Loss classification: 6.337183176787359\n",
      "Iteration 5/40, Loss: 1896.5683182097537\n",
      "Iteration 5/40, Loss classification: 6.300202271379415\n",
      "Iteration 6/40, Loss: 1873.8521175368635\n",
      "Iteration 6/40, Loss classification: 5.732891752703863\n",
      "Iteration 7/40, Loss: 1890.6294320298841\n",
      "Iteration 7/40, Loss classification: 5.50324983618307\n",
      "Iteration 8/40, Loss: 1876.7440234996304\n",
      "Iteration 8/40, Loss classification: 4.972677652460729\n",
      "Iteration 9/40, Loss: 1882.4942825991957\n",
      "Iteration 9/40, Loss classification: 4.6847458487737015\n",
      "Iteration 10/40, Loss: 1885.1909558048778\n",
      "Iteration 10/40, Loss classification: 4.569767671912364\n",
      "Iteration 11/40, Loss: 1871.111570892959\n",
      "Iteration 11/40, Loss classification: 4.511543543187102\n",
      "Iteration 12/40, Loss: 1889.3608812398245\n",
      "Iteration 12/40, Loss classification: 4.314215999893311\n",
      "Iteration 13/40, Loss: 1876.0072272319903\n",
      "Iteration 13/40, Loss classification: 3.9640275230867204\n",
      "Iteration 14/40, Loss: 1866.1968254487024\n",
      "Iteration 14/40, Loss classification: 3.812111403382652\n",
      "Iteration 15/40, Loss: 1886.489597034243\n",
      "Iteration 15/40, Loss classification: 3.821398078362891\n",
      "Iteration 16/40, Loss: 1892.2504002071728\n",
      "Iteration 16/40, Loss classification: 3.6642015510206214\n",
      "Iteration 17/40, Loss: 1882.2716937378104\n",
      "Iteration 17/40, Loss classification: 3.6309544246479706\n",
      "Iteration 18/40, Loss: 1876.2250828203116\n",
      "Iteration 18/40, Loss classification: 3.6967673583337906\n",
      "Iteration 19/40, Loss: 1875.5827309108354\n",
      "Iteration 19/40, Loss classification: 3.2970486667451886\n",
      "Iteration 20/40, Loss: 1874.2911972871923\n",
      "Iteration 20/40, Loss classification: 3.334222410549263\n",
      "Iteration 21/40, Loss: 1869.3458796608893\n",
      "Iteration 21/40, Loss classification: 3.3341342479123504\n",
      "Iteration 22/40, Loss: 1879.4334879889852\n",
      "Iteration 22/40, Loss classification: 3.374628075263346\n",
      "Iteration 23/40, Loss: 1886.4783852896533\n",
      "Iteration 23/40, Loss classification: 3.2578269950932652\n",
      "Iteration 24/40, Loss: 1866.9478886029272\n",
      "Iteration 24/40, Loss classification: 3.0187006683619124\n",
      "Iteration 25/40, Loss: 1857.5159121092756\n",
      "Iteration 25/40, Loss classification: 3.0607473747316587\n",
      "Iteration 26/40, Loss: 1880.2721654403306\n",
      "Iteration 26/40, Loss classification: 2.8977796886390608\n",
      "Iteration 27/40, Loss: 1879.4894038786556\n",
      "Iteration 27/40, Loss classification: 3.162189452842777\n",
      "Iteration 28/40, Loss: 1855.809074028527\n",
      "Iteration 28/40, Loss classification: 2.739946713209148\n",
      "Iteration 29/40, Loss: 1895.0477836970717\n",
      "Iteration 29/40, Loss classification: 2.8401666880422383\n",
      "Iteration 30/40, Loss: 1882.498330954922\n",
      "Iteration 30/40, Loss classification: 2.5856695395948366\n",
      "Iteration 31/40, Loss: 1868.477718702806\n",
      "Iteration 31/40, Loss classification: 2.757134963772776\n",
      "Iteration 32/40, Loss: 1873.9328066076268\n",
      "Iteration 32/40, Loss classification: 2.543885779608637\n",
      "Iteration 33/40, Loss: 1867.571416058367\n",
      "Iteration 33/40, Loss classification: 2.4665367442719286\n",
      "Iteration 34/40, Loss: 1872.959257807301\n",
      "Iteration 34/40, Loss classification: 2.7471560978285448\n",
      "Iteration 35/40, Loss: 1884.5563643788198\n",
      "Iteration 35/40, Loss classification: 2.598918424253619\n",
      "Iteration 36/40, Loss: 1869.7387512623561\n",
      "Iteration 36/40, Loss classification: 2.500614272527848\n",
      "Iteration 37/40, Loss: 1860.9996634128481\n",
      "Iteration 37/40, Loss classification: 2.514111742954448\n",
      "Iteration 38/40, Loss: 1870.9330465422515\n",
      "Iteration 38/40, Loss classification: 2.566833346832292\n",
      "Iteration 39/40, Loss: 1871.342459246487\n",
      "Iteration 39/40, Loss classification: 2.4906163062901574\n",
      "Iteration 40/40, Loss: 1866.4984406888923\n",
      "Iteration 40/40, Loss classification: 2.5071588682096952\n",
      "Train accuracy: 0.5285714285714286\n",
      "Test accuracy: 0.44333333333333336\n",
      "Ground truth labels: [ 1  1 -1 -1 -1 -1 -1 -1 -1 -1]\n",
      "Predicted labels: [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "Number of right predictions: 133\n"
     ]
    }
   ],
   "source": [
    "# do a train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "# Fit the model to the data\n",
    "sdl.fit(X_train, y_train)\n",
    "\n",
    "# do the prediction\n",
    "y_pred_train = sdl.predict(X_train)\n",
    "print(\"Train accuracy:\", np.mean(np.round(y_pred_train) == y_train))\n",
    "\n",
    "y_pred_test = sdl.predict(X_test)\n",
    "print(\"Test accuracy:\", np.mean(np.round(y_pred_test) == y_test))\n",
    "print(\"Ground truth labels:\", y_test[:10])\n",
    "print(\"Predicted labels:\", np.round(y_pred_test)[:10])\n",
    "\n",
    "# count the number of right predictions\n",
    "print(\"Number of right predictions:\", np.sum(np.round(y_pred_test) == y_test))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ts",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
